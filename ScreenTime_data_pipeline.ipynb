{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jzheng23/colab/blob/main/ScreenTime_data_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook can perform the following operations in order:\n",
        "\n",
        "1. Import data from Firebase and Qualtrics directly with API\n",
        "2. Save the data frames as temporary csv files\n",
        "3. Open the temporary csv files and save them to google drive"
      ],
      "metadata": {
        "id": "vgvNYTT3RIl4"
      },
      "id": "vgvNYTT3RIl4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparation"
      ],
      "metadata": {
        "id": "G4pMs-nbRIYx"
      },
      "id": "G4pMs-nbRIYx"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount Google Drive and set up file path"
      ],
      "metadata": {
        "id": "IHGd2zfFRtjA"
      },
      "id": "IHGd2zfFRtjA"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3EbblXTRf6I",
        "outputId": "47bef2e9-4f24-4087-bb59-2a87f20495c2"
      },
      "id": "l3EbblXTRf6I",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the Google Drive path, depending who is running the notebook"
      ],
      "metadata": {
        "id": "-egH6q8BtHV5"
      },
      "id": "-egH6q8BtHV5"
    },
    {
      "cell_type": "code",
      "source": [
        "#Jian\n",
        "google_drive_data_path = '/content/drive/MyDrive/Problematic smartphone usage/Ambient display/Data'\n",
        "google_drive_key_path = '/content/drive/MyDrive/Problematic smartphone usage/Ambient display/Key'"
      ],
      "metadata": {
        "id": "cdizntv5qCsp"
      },
      "id": "cdizntv5qCsp",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Subin\n",
        "google_drive_data_path = '/content/drive/MyDrive/UMD_research/Problematic_Smartphone_Usage'\n",
        "google_drive_key_path = '/content/drive/MyDrive/UMD_research/Problematic_Smartphone_Usage'"
      ],
      "metadata": {
        "id": "dAxjtUZqKMm7"
      },
      "id": "dAxjtUZqKMm7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Firebase database"
      ],
      "metadata": {
        "id": "Z-x7WGXf-Uuf"
      },
      "id": "Z-x7WGXf-Uuf"
    },
    {
      "cell_type": "code",
      "source": [
        "import firebase_admin\n",
        "from firebase_admin import credentials, db\n",
        "import csv\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "zy1HrBmDnOJx"
      },
      "id": "zy1HrBmDnOJx",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Don't re-run this cell unless the kernel has been restarted\n",
        "cred = credentials.Certificate(google_drive_key_path+'/timer-42ad2-firebase-adminsdk-4r7oj-2c373565f2.json')\n",
        "firebase_admin.initialize_app(cred, {\n",
        "    'databaseURL': 'https://timer-42ad2-default-rtdb.firebaseio.com'\n",
        "})"
      ],
      "metadata": {
        "id": "gFVB47CUKTsr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "338e1709-537a-41c9-9333-a28d05de6141"
      },
      "id": "gFVB47CUKTsr",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<firebase_admin.App at 0x7aac3c30a7a0>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert Unix timestamp to date/datetime in specified timezone\n",
        "def convert_unix_to_date(unix_timestamp, timezone='America/New_York', return_date=True):\n",
        "   \"\"\"\n",
        "\n",
        "   Parameters:\n",
        "       unix_timestamp: Unix timestamp in milliseconds\n",
        "       timezone: String of timezone (default 'America/New_York')\n",
        "       return_date: If True returns date only, if False returns datetime\n",
        "   \"\"\"\n",
        "   dt = pd.to_datetime(unix_timestamp, unit='ms', utc=True).tz_convert(timezone)\n",
        "   return dt.date() if return_date else dt"
      ],
      "metadata": {
        "id": "QQdQsFQSslsy"
      },
      "id": "QQdQsFQSslsy",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Response data"
      ],
      "metadata": {
        "id": "EQ-a_Ljo7Wm_"
      },
      "id": "EQ-a_Ljo7Wm_"
    },
    {
      "cell_type": "code",
      "source": [
        "def process_responses(responses, pid, host, survey_id):\n",
        "    responses_data = []\n",
        "    response_dict = {\n",
        "        'pid': pid,\n",
        "        'host': host,\n",
        "        'surveyID': survey_id\n",
        "    }\n",
        "\n",
        "    if isinstance(responses, dict):\n",
        "        for question_id, answer in responses.items():\n",
        "            response_dict[f'q_{question_id}'] = answer\n",
        "    elif isinstance(responses, list):\n",
        "        for question_id, answer in enumerate(responses):\n",
        "            if answer is not None:\n",
        "                response_dict[f'q_{question_id}'] = answer\n",
        "\n",
        "    responses_data.append(response_dict)\n",
        "    return responses_data\n",
        "\n",
        "def get_timestamp(data):\n",
        "    if isinstance(data, dict):\n",
        "        return data.get('a') or data.get('timestamp')\n",
        "    return None\n",
        "\n",
        "def get_responses(data):\n",
        "    if isinstance(data, dict):\n",
        "        return data.get('b') or data.get('responses')\n",
        "    elif isinstance(data, list):\n",
        "        return data\n",
        "    return None\n",
        "\n",
        "def firebase_to_csv_log_Screen(ref_path, output_file):\n",
        "    ref = db.reference(ref_path)\n",
        "    data = ref.get()\n",
        "    transformed_data = []\n",
        "\n",
        "    for pid, pid_data in data.items():\n",
        "        pid = pid.lower()\n",
        "\n",
        "        # Process each host under the pid\n",
        "        for host, events in pid_data.items():\n",
        "            if isinstance(events, list):\n",
        "                for index, event in enumerate(events):\n",
        "                    if event is not None and isinstance(event, dict):\n",
        "                        row = {\n",
        "                            'pid': pid,\n",
        "                            'host': host,\n",
        "                            'eventLabel': index,\n",
        "                            'startTime': event.get('startTime'),\n",
        "                            'endTime': event.get('endTime')\n",
        "                        }\n",
        "                        transformed_data.append(row)\n",
        "\n",
        "    df = pd.DataFrame(transformed_data)\n",
        "    if len(transformed_data) > 0:\n",
        "        df = df[['pid', 'host', 'eventLabel', 'startTime', 'endTime']]\n",
        "    df.to_csv(output_file, index=False)\n",
        "\n",
        "def responses_to_csvs(meta_file, responses_file):\n",
        "    ref = db.reference('/responses')\n",
        "    data = ref.get()\n",
        "\n",
        "    meta_data = []\n",
        "    responses_data = []\n",
        "\n",
        "    for pid, pid_data in data.items():\n",
        "        pid = pid.lower()\n",
        "\n",
        "        # Process each host under pid\n",
        "        for host, host_data in pid_data.items():\n",
        "            # If host_data is a list, enumerate through it\n",
        "            if isinstance(host_data, list):\n",
        "                for survey_id, survey_data in enumerate(host_data):\n",
        "                    if survey_data is not None:\n",
        "                        timestamp = get_timestamp(survey_data)\n",
        "                        if timestamp:\n",
        "                            meta_data.append({\n",
        "                                'pid': pid,\n",
        "                                'host': host,\n",
        "                                'surveyID': str(survey_id),\n",
        "                                'timestamp': timestamp\n",
        "                            })\n",
        "\n",
        "                            responses = get_responses(survey_data)\n",
        "                            if responses:\n",
        "                                responses_data.extend(process_responses(responses, pid, host, str(survey_id)))\n",
        "\n",
        "    # Create and save metadata DataFrame\n",
        "    meta_df = pd.DataFrame(meta_data)\n",
        "    if len(meta_data) > 0:\n",
        "        meta_df = meta_df[['pid', 'host', 'surveyID', 'timestamp']]\n",
        "    meta_df.to_csv(meta_file, index=False)\n",
        "\n",
        "    # Create responses DataFrame in wide format\n",
        "    responses_df = pd.DataFrame(responses_data)\n",
        "\n",
        "    if len(responses_data) > 0:\n",
        "        first_cols = ['pid', 'host', 'surveyID']\n",
        "        q_cols = [col for col in responses_df.columns if col.startswith('q_')]\n",
        "        q_cols.sort(key=lambda x: int(x.split('_')[1]))\n",
        "        responses_df = responses_df[first_cols + q_cols]\n",
        "\n",
        "    responses_df.to_csv(responses_file, index=False)"
      ],
      "metadata": {
        "id": "Q4q0ytetcZIu"
      },
      "id": "Q4q0ytetcZIu",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "responses_to_csvs('survey_meta.csv', 'survey_responses.csv')\n",
        "survey_meta_data = pd.read_csv('/content/survey_meta.csv')\n",
        "print(\"The shape of the survey_meta_data is \" + str(survey_meta_data.shape))\n",
        "survey_meta_data.to_csv(google_drive_data_path + '/survey_meta.csv', index=False)\n",
        "survey_responses_data = pd.read_csv('/content/survey_responses.csv')\n",
        "print(\"The shape of the survey_responses_data is \" + str(survey_responses_data.shape))\n",
        "survey_responses_data.to_csv(google_drive_data_path + '/survey_responses.csv', index=False)"
      ],
      "metadata": {
        "id": "yTWJeIa2Bdoi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "503c82a2-f0db-40ae-c43c-4b8c588a7b06"
      },
      "id": "yTWJeIa2Bdoi",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The shape of the survey_meta_data is (17, 4)\n",
            "The shape of the survey_responses_data is (17, 33)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*TODO* still cannot read the host\n",
        "\n",
        "In the real data there should always be a host"
      ],
      "metadata": {
        "id": "HLHOV_udHoPp"
      },
      "id": "HLHOV_udHoPp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "This should work for the real data, not tested yet\n",
        "\n",
        "Select all the lines and use Ctrl + / to uncomment"
      ],
      "metadata": {
        "id": "BkMeOVklIPMB"
      },
      "id": "BkMeOVklIPMB"
    },
    {
      "cell_type": "code",
      "source": [
        "# def process_responses(responses, pid, host, survey_id):\n",
        "#     responses_data = []\n",
        "#     response_dict = {\n",
        "#         'pid': pid,\n",
        "#         'host': host,\n",
        "#         'surveyID': survey_id\n",
        "#     }\n",
        "\n",
        "#     if isinstance(responses, dict):\n",
        "#         for question_id, answer in responses.items():\n",
        "#             response_dict[f'q_{question_id}'] = answer\n",
        "#     elif isinstance(responses, list):\n",
        "#         for question_id, answer in enumerate(responses):\n",
        "#             if answer is not None:\n",
        "#                 response_dict[f'q_{question_id}'] = answer\n",
        "\n",
        "#     responses_data.append(response_dict)\n",
        "#     return responses_data\n",
        "\n",
        "# def get_timestamp(data):\n",
        "#     if isinstance(data, dict):\n",
        "#         return data.get('a') or data.get('timestamp')\n",
        "#     return None\n",
        "\n",
        "# def get_responses(data):\n",
        "#     if isinstance(data, dict):\n",
        "#         return data.get('b') or data.get('responses')\n",
        "#     elif isinstance(data, list):\n",
        "#         return data\n",
        "#     return None\n",
        "\n",
        "# def responses_to_csvs(meta_file, responses_file):\n",
        "#     ref = db.reference('/responses')\n",
        "#     data = ref.get()\n",
        "\n",
        "#     meta_data = []\n",
        "#     responses_data = []\n",
        "\n",
        "#     for pid, pid_data in data.items():\n",
        "#         if not pid_data:\n",
        "#             continue\n",
        "\n",
        "#         # Process each host under pid\n",
        "#         for host, host_data in pid_data.items():\n",
        "#             # Process each survey under host\n",
        "#             for survey_id, survey_data in host_data.items():\n",
        "#                 timestamp = get_timestamp(survey_data)\n",
        "\n",
        "#                 meta_data.append({\n",
        "#                     'pid': pid,\n",
        "#                     'host': host,\n",
        "#                     'surveyID': survey_id,\n",
        "#                     'timestamp': timestamp\n",
        "#                 })\n",
        "\n",
        "#                 responses = get_responses(survey_data)\n",
        "#                 if responses:\n",
        "#                     responses_data.extend(process_responses(responses, pid, host, survey_id))\n",
        "\n",
        "#     # Create and save metadata DataFrame\n",
        "#     meta_df = pd.DataFrame(meta_data)\n",
        "#     meta_df = meta_df[['pid', 'host', 'surveyID', 'timestamp']]  # ensure column order\n",
        "#     meta_df.to_csv(meta_file, index=False)\n",
        "\n",
        "#     # Create responses DataFrame in wide format\n",
        "#     responses_df = pd.DataFrame(responses_data)\n",
        "\n",
        "#     # Ensure the first columns are in the correct order\n",
        "#     first_cols = ['pid', 'host', 'surveyID']\n",
        "\n",
        "#     # Get question columns and sort them numerically\n",
        "#     q_cols = [col for col in responses_df.columns if col.startswith('q_')]\n",
        "#     q_cols.sort(key=lambda x: int(x.split('_')[1]))\n",
        "\n",
        "#     # Combine columns in correct order\n",
        "#     responses_df = responses_df[first_cols + q_cols]\n",
        "\n",
        "#     responses_df.to_csv(responses_file, index=False)"
      ],
      "metadata": {
        "id": "rpkligPVIOxd"
      },
      "id": "rpkligPVIOxd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Screen events"
      ],
      "metadata": {
        "id": "BgIVGzJj7cqO"
      },
      "id": "BgIVGzJj7cqO"
    },
    {
      "cell_type": "code",
      "source": [
        "# def firebase_to_csv_log_Screen (ref_path, output_file):\n",
        "#     ref = db.reference(ref_path)\n",
        "#     data = ref.get()\n",
        "#     transformed_data = []\n",
        "\n",
        "#     # dictionary structure\n",
        "#     for pid, events in data.items():  # 'pid' = key\n",
        "#         pid = pid.lower()\n",
        "#         if isinstance(events, list):\n",
        "#             for index, event in enumerate(events, start=0):  #event_label_start:1\n",
        "#                 if event is None:\n",
        "#                     continue\n",
        "#                 if isinstance(event, dict):\n",
        "#                     row = {'pid': pid,\n",
        "#                         'eventLabel': index,\n",
        "#                         'startTime': event.get('startTime'),\n",
        "#                         'endTime': event.get('endTime')}\n",
        "#                     transformed_data.append(row)\n",
        "\n",
        "#     df = pd.DataFrame(transformed_data)\n",
        "#     df = df[['pid', 'eventLabel', 'startTime', 'endTime']]\n",
        "#     df.to_csv(output_file, index=False)"
      ],
      "metadata": {
        "id": "u_U08auqLWlk"
      },
      "id": "u_U08auqLWlk",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def firebase_to_csv_log_Screen(ref_path, output_file):\n",
        "    ref = db.reference(ref_path)\n",
        "    data = ref.get()\n",
        "    transformed_data = []\n",
        "\n",
        "    # dictionary structure\n",
        "    for pid, pid_data in data.items():\n",
        "        pid = pid.lower()\n",
        "\n",
        "        # Skip if pid_data is not a dictionary (to handle old format entries)\n",
        "        if not isinstance(pid_data, dict):\n",
        "            continue\n",
        "\n",
        "        # Process each host under the pid\n",
        "        for host, events in pid_data.items():\n",
        "            if not isinstance(events, list):\n",
        "                continue\n",
        "\n",
        "            for index, event in enumerate(events, start=0):\n",
        "                if event is None:\n",
        "                    continue\n",
        "                if isinstance(event, dict):\n",
        "                    row = {\n",
        "                        'pid': pid,\n",
        "                        'host': host,\n",
        "                        'eventLabel': index,\n",
        "                        'startTime': event.get('startTime'),\n",
        "                        'endTime': event.get('endTime')\n",
        "                    }\n",
        "                    transformed_data.append(row)\n",
        "\n",
        "    df = pd.DataFrame(transformed_data)\n",
        "    if len(transformed_data) > 0:\n",
        "        df = df[['pid', 'host', 'eventLabel', 'startTime', 'endTime']]\n",
        "    df.to_csv(output_file, index=False)"
      ],
      "metadata": {
        "id": "_j9kjFxsa6PV"
      },
      "id": "_j9kjFxsa6PV",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "firebase_to_csv_log_Screen('/screen_events','screen_events.csv')\n",
        "screen_events_data = pd.read_csv('/content/screen_events.csv')\n",
        "print(\"The shape of the screen_events_data is \" + str(screen_events_data.shape))\n",
        "screen_events_data.to_csv(google_drive_data_path + '/screen_events.csv', index=False)"
      ],
      "metadata": {
        "id": "CLCIRK2H7qgu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96d5a130-586b-4758-8e81-9895383bec9e"
      },
      "id": "CLCIRK2H7qgu",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The shape of the screen_events_data is (1340, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting changes"
      ],
      "metadata": {
        "id": "gPZfk9lA7e8f"
      },
      "id": "gPZfk9lA7e8f"
    },
    {
      "cell_type": "code",
      "source": [
        "def firebase_to_csv_log_setting (ref_path, output_file):\n",
        "    ref = db.reference(ref_path)\n",
        "    data = ref.get()\n",
        "    transformed_data = []\n",
        "\n",
        "    # dictionary structure\n",
        "    for pid, changes in data.items():  # 'pid' = key\n",
        "        if isinstance(changes, list):\n",
        "            for index, change in enumerate(changes, start=0):  #event_label_start:1\n",
        "                if change is None:\n",
        "                    continue\n",
        "                if isinstance(change, dict):\n",
        "                    row = {'pid': pid,\n",
        "                        'eventLabel': index,\n",
        "                        'newValue' : change.get('newValue'),\n",
        "                        'setting': change.get('setting'),\n",
        "                        'timestamp': change.get('timestamp')}\n",
        "                    transformed_data.append(row)\n",
        "\n",
        "    df = pd.DataFrame(transformed_data)\n",
        "    df = df[['pid', 'newValue', 'setting', 'timestamp']]\n",
        "    df.to_csv(output_file, index=False)"
      ],
      "metadata": {
        "id": "7wKmjI3kdlhZ"
      },
      "id": "7wKmjI3kdlhZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "firebase_to_csv_log_setting('/settings_change_logs','settings_change_logs.csv')\n",
        "settings_change_logs_data = pd.read_csv('/content/settings_change_logs.csv')\n",
        "print(\"The shape of the settings_change_logs_data is \" + str(settings_change_logs_data.shape))\n",
        "settings_change_logs_data.to_csv(google_drive_data_path + '/settings_change_logs.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4BKVzrla3cL",
        "outputId": "288bd72c-c85d-4fa7-d912-83819a0e2adf"
      },
      "id": "z4BKVzrla3cL",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The shape of the settings_change_logs_data is (21, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ringer"
      ],
      "metadata": {
        "id": "m4Meu_IQiuts"
      },
      "id": "m4Meu_IQiuts"
    },
    {
      "cell_type": "code",
      "source": [
        "def firebase_to_csv_log_mode(ref_path, output_file):\n",
        "    ref = db.reference(ref_path)\n",
        "    data = ref.get()\n",
        "    transformed_data = []\n",
        "\n",
        "    for pid, middle_levels in data.items():\n",
        "        pid = pid.lower()\n",
        "        for middle_key, sub_changes in middle_levels.items():\n",
        "            for index, event in enumerate(sub_changes):\n",
        "                if isinstance(event, dict):\n",
        "                    row = {'pid': pid,'host':middle_key,'label': str(index),\n",
        "                        'mode': event.get('mode'),\n",
        "                        'timestamp': event.get('timestamp')}\n",
        "                    transformed_data.append(row)\n",
        "\n",
        "    df = pd.DataFrame(transformed_data)\n",
        "    df = df[['pid', 'host', 'label', 'mode', 'timestamp']]\n",
        "    df.to_csv(output_file, index=False)"
      ],
      "metadata": {
        "id": "NkA5rfhwv1Ga"
      },
      "id": "NkA5rfhwv1Ga",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "firebase_to_csv_log_mode('/ringer_mode_events','ringer_mode_events.csv')\n",
        "ringer_mode_events_data = pd.read_csv('/content/ringer_mode_events.csv')\n",
        "print(\"The shape of the ringer_mode_events_data is \" + str(ringer_mode_events_data.shape))\n",
        "ringer_mode_events_data.to_csv(google_drive_data_path + '/ringer_mode_events.csv', index=False)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "KbCPCVA6iud8"
      },
      "id": "KbCPCVA6iud8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ringer_mode_events_data0['label'] = ringer_mode_events_data0.groupby('pid').cumcount() + 1\n",
        "# ringer_mode_events_data0"
      ],
      "metadata": {
        "id": "41RkSPqx75hm"
      },
      "id": "41RkSPqx75hm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Device info"
      ],
      "metadata": {
        "id": "2u0oTRRn5593"
      },
      "id": "2u0oTRRn5593"
    },
    {
      "cell_type": "code",
      "source": [
        "# this function works for device and timezone infor\n",
        "def firebase_to_csv(ref_path, output_file):\n",
        "    # Get reference to device_info\n",
        "    ref = db.reference(ref_path)\n",
        "    data = ref.get()\n",
        "\n",
        "    transformed_data = []\n",
        "\n",
        "    # Transform the nested structure\n",
        "    for pid, pid_data in data.items():\n",
        "        pid = pid.lower()\n",
        "\n",
        "        if isinstance(pid_data, str):\n",
        "            # Case where timezone is directly under pid\n",
        "            row = {\n",
        "                'pid': pid,\n",
        "                'host': pd.NA,  # or None\n",
        "                'timezone': pid_data\n",
        "            }\n",
        "            transformed_data.append(row)\n",
        "        else:\n",
        "            # Case where pid has host-timezone pairs\n",
        "            for host, timezone in pid_data.items():\n",
        "                row = {\n",
        "                    'pid': pid,\n",
        "                    'host': host,\n",
        "                    'timezone': timezone\n",
        "                }\n",
        "                transformed_data.append(row)\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    df = pd.DataFrame(transformed_data)\n",
        "\n",
        "    # Ensure pid and host are the first columns\n",
        "    cols = ['pid', 'host'] + [col for col in df.columns if col not in ['pid', 'host']]\n",
        "    df = df[cols]\n",
        "\n",
        "    # Save to CSV\n",
        "    df.to_csv(output_file, index=False)"
      ],
      "metadata": {
        "id": "TAhF2VZv5Zr1"
      },
      "id": "TAhF2VZv5Zr1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "firebase_to_csv('/device_info','device_info.csv')\n",
        "device_data = pd.read_csv('/content/device_info.csv')\n",
        "print(\"The shape of the device_data0 is \" + str(device_data.shape))\n",
        "device_data.to_csv(google_drive_data_path + '/device_info.csv', index=False)"
      ],
      "metadata": {
        "id": "OmCRGkAJIVWR"
      },
      "id": "OmCRGkAJIVWR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Timezone info"
      ],
      "metadata": {
        "id": "9c66xeLT7nfI"
      },
      "id": "9c66xeLT7nfI"
    },
    {
      "cell_type": "code",
      "source": [
        "firebase_to_csv('/timezones','timezones.csv')\n",
        "timezones_data = pd.read_csv('/content/timezones.csv')\n",
        "print(\"The shape of the timezones_data is \" + str(timezones_data.shape))\n",
        "timezones_data.to_csv(google_drive_data_path + '/timezones.csv', index=False)"
      ],
      "metadata": {
        "id": "do7aFA789Lzw"
      },
      "id": "do7aFA789Lzw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Counters"
      ],
      "metadata": {
        "id": "2vwb_EftYtPp"
      },
      "id": "2vwb_EftYtPp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We don't actually need top analyze the counter data"
      ],
      "metadata": {
        "id": "HrOus7dZ8BjX"
      },
      "id": "HrOus7dZ8BjX"
    },
    {
      "cell_type": "code",
      "source": [
        "# def firebase_to_csv_counters(ref_path, output_file):\n",
        "#     ref = db.reference(ref_path)\n",
        "#     data = ref.get()\n",
        "#     transformed_data = []\n",
        "#     for pid, value in data.items():\n",
        "#         pid = pid.lower()\n",
        "#         # Case 1: only numeric\n",
        "#         if isinstance(value, int):\n",
        "#             transformed_data.append({'pid': pid, 'screen_event_count': value})\n",
        "\n",
        "#         # Case 2: dictionary => selecting only value\n",
        "#         elif isinstance(value, dict):\n",
        "#             numeric_values = [v for v in value.values() if isinstance(v, int)]\n",
        "#             if numeric_values:\n",
        "#                 total = sum(numeric_values)\n",
        "#                 transformed_data.append({'pid': pid, 'screen_event_count': total})\n",
        "\n",
        "#     df = pd.DataFrame(transformed_data)\n",
        "#     df = df[['pid', 'screen_event_count']]\n",
        "#     df.to_csv(output_file, index=False)\n",
        "\n",
        "\n",
        "# firebase_to_csv_counters('/ringer_event_counters','ringer_event_counters.csv')\n",
        "# ringer_event_counters_data0 = pd.read_csv('/content/ringer_event_counters.csv')\n",
        "# print(ringer_event_counters_data0.head())\n",
        "\n",
        "# firebase_to_csv_counters('/screen_event_counters','screen_event_counters.csv')\n",
        "# screen_event_counters_data0 = pd.read_csv('/content/screen_event_counters.csv')\n",
        "# print(screen_event_counters_data0.head())\n",
        "\n",
        "# firebase_to_csv_counters('/settings_change_counters','settings_change_counters.csv')\n",
        "# settings_change_counters_data0 = pd.read_csv('/content/settings_change_counters.csv')\n",
        "# print(settings_change_counters_data0.head())"
      ],
      "metadata": {
        "id": "fk6_muXpjtZx"
      },
      "id": "fk6_muXpjtZx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Qualtrics data"
      ],
      "metadata": {
        "id": "GXkpBVhZ-mUA"
      },
      "id": "GXkpBVhZ-mUA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sign-up survey"
      ],
      "metadata": {
        "id": "9wPrJCrVsmBq"
      },
      "id": "9wPrJCrVsmBq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read and save the raw data"
      ],
      "metadata": {
        "id": "vm69YDjX93BA"
      },
      "id": "vm69YDjX93BA"
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import json\n",
        "import time\n",
        "import io\n",
        "\n",
        "def get_qualtrics_data(api_token, survey_id):\n",
        "    # API configurations\n",
        "    base_url = f\"https://pdx1.qualtrics.com/API/v3/surveys/{survey_id}/export-responses\"\n",
        "    headers = {\n",
        "        \"X-API-TOKEN\": api_token,\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    # Start export\n",
        "    export_payload = '{\"format\":\"csv\"}'\n",
        "    export_response = requests.post(base_url, data=export_payload, headers=headers)\n",
        "    export_progress_id = export_response.json()[\"result\"][\"progressId\"]\n",
        "\n",
        "    # Check export progress\n",
        "    while True:\n",
        "        progress_response = requests.get(\n",
        "            f\"{base_url}/{export_progress_id}\",\n",
        "            headers=headers\n",
        "        )\n",
        "        progress_status = progress_response.json()[\"result\"][\"status\"]\n",
        "\n",
        "        if progress_status == \"complete\":\n",
        "            file_id = progress_response.json()[\"result\"][\"fileId\"]\n",
        "            break\n",
        "        time.sleep(2)\n",
        "\n",
        "    # Download file\n",
        "    download_response = requests.get(\n",
        "        f\"{base_url}/{file_id}/file\",\n",
        "        headers=headers\n",
        "    )\n",
        "\n",
        "    # Extract zip file\n",
        "    with zipfile.ZipFile(io.BytesIO(download_response.content)) as zip_file:\n",
        "        return zip_file.read(zip_file.namelist()[0]).decode('utf-8')\n"
      ],
      "metadata": {
        "id": "KQJtch0aeAI_"
      },
      "id": "KQJtch0aeAI_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sign_up_survey_pilot = \"SV_dgN8IwiCIfglbAq\"\n",
        "\n",
        "sign_up_survey = \"SV_3RiDob4GtY8kCSG\"\n",
        "api_token = \"U5xGlZmJv76LsjIXvfwB7FS9RqrqwmMb3vva3pbD\""
      ],
      "metadata": {
        "id": "aRlntIsPfArY"
      },
      "id": "aRlntIsPfArY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 10S\n",
        "import pandas as pd\n",
        "\n",
        "signup_survey_data = get_qualtrics_data(api_token, sign_up_survey) # remove _pilot if running on the real data\n",
        "# Then save to CSV\n",
        "with open('signup_survey_data.csv', 'w') as f:\n",
        "    f.write(signup_survey_data)\n",
        "    f.close()\n",
        "\n",
        "signup_survey_data = pd.read_csv('signup_survey_data.csv')\n",
        "print(\"The shape of the signup_survey_data is \" + str(signup_survey_data.shape))\n",
        "signup_survey_data.to_csv(google_drive_data_path + '/signup_survey_data_raw.csv', index=False)"
      ],
      "metadata": {
        "id": "xgrZdnIDZ4th",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed99c23b-356e-4cd5-c098-aa88b5c5bad5"
      },
      "id": "xgrZdnIDZ4th",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The shape of the signup_survey_data is (526, 55)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Process the data"
      ],
      "metadata": {
        "id": "_n6nv8zK97aA"
      },
      "id": "_n6nv8zK97aA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get city from location"
      ],
      "metadata": {
        "id": "V196aG8r0I4F"
      },
      "id": "V196aG8r0I4F"
    },
    {
      "cell_type": "code",
      "source": [
        "signup_data0 = pd.read_csv(google_drive_data_path + '/signup_survey_data_raw.csv')\n",
        "signup_data = signup_data0.copy()\n",
        "#drop row 2 & 3\n",
        "signup_data = signup_data.drop([0, 1])"
      ],
      "metadata": {
        "id": "W0Xv7m_w1J5Q"
      },
      "id": "W0Xv7m_w1J5Q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# install the package if missing\n",
        "!pip install reverse_geocoder"
      ],
      "metadata": {
        "id": "3LkPouJc19RB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4333fe98-ff21-448c-9538-f5c8e3e28018"
      },
      "id": "3LkPouJc19RB",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting reverse_geocoder\n",
            "  Downloading reverse_geocoder-1.5.1.tar.gz (2.2 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/2.2 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m1.9/2.2 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from reverse_geocoder) (1.26.4)\n",
            "Requirement already satisfied: scipy>=0.17.1 in /usr/local/lib/python3.10/dist-packages (from reverse_geocoder) (1.13.1)\n",
            "Building wheels for collected packages: reverse_geocoder\n",
            "  Building wheel for reverse_geocoder (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for reverse_geocoder: filename=reverse_geocoder-1.5.1-py3-none-any.whl size=2268068 sha256=4c59ab0f953297f138c7078a93fd37221708695939f9a598b768980b7cfffd9d\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/e5/88/eb139b6d6a26b8022d370ab991f7a836802fed9871975ec6d9\n",
            "Successfully built reverse_geocoder\n",
            "Installing collected packages: reverse_geocoder\n",
            "Successfully installed reverse_geocoder-1.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import reverse_geocoder as rg\n",
        "\n",
        "def get_country(lat, lon):\n",
        "    try:\n",
        "        result = rg.search((lat, lon))\n",
        "        if result:\n",
        "            return result[0]['cc']  # returns country code\n",
        "        return 'Unknown'\n",
        "    except:\n",
        "        return 'Unknown'"
      ],
      "metadata": {
        "id": "vfWFFi6C0Mfj"
      },
      "id": "vfWFFi6C0Mfj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get country\n",
        "signup_data['Country'] = signup_data.apply(\n",
        "    lambda row: get_country(row['LocationLatitude'], row['LocationLongitude']),\n",
        "    axis=1\n",
        ")"
      ],
      "metadata": {
        "id": "clR_T-q14eq1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e651a7bf-57e1-4f1d-e1a6-e50625f7e4bc"
      },
      "id": "clR_T-q14eq1",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading formatted geocoded file...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "signup_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wb_xB5yhInpV",
        "outputId": "ff37712d-68b1-4245-f1f6-59f9422f8fad"
      },
      "id": "wb_xB5yhInpV",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(524, 56)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: in signup_data, drop rows where Q19 is empty\n",
        "\n",
        "# Drop rows where 'Q19' is empty\n",
        "signup_data1 = signup_data.dropna(subset=['Q19'])\n",
        "signup_data1.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtIDsFnrIR-i",
        "outputId": "17672834-594e-4f48-a585-3ca87c7adc84"
      },
      "id": "UtIDsFnrIR-i",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(462, 56)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: in signup_data, create a new column \"valid\" with default value 1, use .loc[]\n",
        "signup_data1.loc[:, 'valid'] = 1\n",
        "\n",
        "# prompt: in signup_data, let \"valid\" be 2 if Country is not US\n",
        "\n",
        "signup_data1.loc[signup_data1['Country'] != 'US', 'valid'] = 2\n",
        "\n",
        "# Create a new column 'location_count' that shows how many times each lat/long pair appears\n",
        "signup_data1['location_count'] = signup_data1.groupby(['LocationLatitude', 'LocationLongitude'])['LocationLatitude'].transform('count')\n",
        "\n",
        "# prompt: in signup_data1, let \"valid\" be 3 if location_count is not 1\n",
        "\n",
        "signup_data1.loc[signup_data1['location_count'] != 1, 'valid'] = 3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ciN12ZQbJd3P",
        "outputId": "58a30743-e4e8-480f-f45b-4156695c5c17"
      },
      "id": "ciN12ZQbJd3P",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-d8c9fc8687df>:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  signup_data1.loc[:, 'valid'] = 1\n",
            "<ipython-input-18-d8c9fc8687df>:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  signup_data1['location_count'] = signup_data1.groupby(['LocationLatitude', 'LocationLongitude'])['LocationLatitude'].transform('count')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: in signup_data1, if there are rows with the same value in Q19, keep only the first one, delete the duplicated.\n",
        "\n",
        "# Remove duplicate rows based on 'Q19', keeping the first occurrence\n",
        "signup_data1 = signup_data1.drop_duplicates(subset=['Q19'], keep='first')\n"
      ],
      "metadata": {
        "id": "AigC-tLrOxXQ"
      },
      "id": "AigC-tLrOxXQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# prompt: in signup_data1, let \"valid\" be 5 where Q_RecaptchaScore < 0.8 or Q_RelevantIDDuplicate = TRUE\n",
        "\n",
        "signup_data1.loc[(signup_data1['Q_RecaptchaScore'].astype(float) < 0.8) | (signup_data1['Q_RelevantIDDuplicate'] == \"TRUE\"), 'valid'] = 5\n",
        "\n",
        "# prompt: in signup_data1, let \"valid\" be 5 where Q_RelevantIDFraudScore is float and greater than 0\n",
        "\n",
        "signup_data1.loc[(signup_data1['Q_RelevantIDFraudScore'].astype(float) > 0), 'valid'] = 5"
      ],
      "metadata": {
        "id": "3fzJN0qcSN7p"
      },
      "id": "3fzJN0qcSN7p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: in signup_data1, let \"valid\" be 1 if the value in the column Q23 contains \"jan\", \"rana\", or \"wechat\", either upper or lower case. I am sure Q23 exits\n",
        "\n",
        "signup_data1.loc[\n",
        "    signup_data1['Q23'].str.contains('jan|rana|wechat', case=False, na=False), 'valid'] = 1\n",
        "\n",
        "    # prompt: in signup_data1, let \"valid\" be 4 if the value in the column Q23 contains \"Facebook\", \"reddit\", or \"flyer\", either upper or lower case. I am sure Q23 exits\n",
        "\n",
        "signup_data1.loc[\n",
        "    signup_data1['Q23'].str.contains('Facebook|reddit|flyer', case=False, na=False), 'valid'] = 4"
      ],
      "metadata": {
        "id": "5dTUjq6ZKIEu"
      },
      "id": "5dTUjq6ZKIEu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "signup_data1.to_csv(google_drive_data_path + '/signup_survey_data_labeled.csv', index=False)"
      ],
      "metadata": {
        "id": "rHFbzfkX3ObO"
      },
      "id": "rHFbzfkX3ObO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First filter for valid rows\n",
        "signup_data2 = signup_data1[signup_data1['valid'] == 1]\n",
        "\n",
        "# Create a dictionary from the CSV mapping\n",
        "column_mapping = {\n",
        "    'Q3': 'Age',\n",
        "    'Q5': 'Brand',\n",
        "    'Q5_5_TEXT': 'Brand_text',\n",
        "    'Q6': 'Android_version',\n",
        "    'Q11': 'Screen_time',\n",
        "    'Q13': 'Gender',\n",
        "    'Q13_4_TEXT': 'Gender_text',\n",
        "    'Q14': 'Education',\n",
        "    'Q15_1': 'Race_white',\n",
        "    'Q15_2': 'Race_black',\n",
        "    'Q15_3': 'Race_native',\n",
        "    'Q15_4': 'Race_Asian',\n",
        "    'Q15_5': 'Race_Pacific',\n",
        "    'Q15_6': 'Race_others',\n",
        "    'Q15_7': 'Race_text',\n",
        "    'Q16_1': 'Employment_retired',\n",
        "    'Q16_2': 'Employment_self-employed',\n",
        "    'Q16_3': 'Employment_employment full-time',\n",
        "    'Q16_4': 'Employment_employment part-time',\n",
        "    'Q16_5': 'Employment_voluntary worker',\n",
        "    'Q16_6': 'Employment_homemaker',\n",
        "    'Q16_7': 'Employment_student',\n",
        "    'Q16_8': 'Employment_others',\n",
        "    'Q16_4_TEXT': 'Employment_hours_text',\n",
        "    'Q16_8_TEXT': 'Employment_text',\n",
        "    'Q18': 'Name',\n",
        "    'Q19': 'Email',\n",
        "    'Q23': 'Source'\n",
        "}\n",
        "\n",
        "# Rename and keep only the specified columns\n",
        "signup_data2 = signup_data2[column_mapping.keys()].rename(columns=column_mapping)"
      ],
      "metadata": {
        "id": "yUIjJ-beQJ86"
      },
      "id": "yUIjJ-beQJ86",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "signup_data2.to_csv(google_drive_data_path + '/signup_survey_data_valid.csv', index=False)"
      ],
      "metadata": {
        "id": "Y81t423fQJT4"
      },
      "id": "Y81t423fQJT4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_signup_data(df):\n",
        "   # Create a copy\n",
        "   df = df.copy()\n",
        "\n",
        "   # Define mappings\n",
        "   mappings = {\n",
        "       'Brand': {'1': 'Samsung', '3': 'Xiaomi', '4': 'Google'},\n",
        "       'Gender': {'1': 'female', '2': 'male', '3': 'non-binary'},\n",
        "       'Education': {\n",
        "           '1': 'some high school or less',\n",
        "           '2': 'high school or GED',\n",
        "           '3': 'some college no degree',\n",
        "           '4': 'associate or technical degree',\n",
        "           '5': 'bachelor',\n",
        "           '6': 'graduate or professional',\n",
        "           '7': 'prefer not to say'\n",
        "       }\n",
        "   }\n",
        "\n",
        "   # Apply mappings with fallback to text columns\n",
        "   df['Brand'] = df.apply(lambda x: mappings['Brand'].get(x['Brand'], x['Brand_text']), axis=1)\n",
        "   df['Gender'] = df.apply(lambda x: mappings['Gender'].get(x['Gender'], x['Gender_text']), axis=1)\n",
        "   df['Education'] = df['Education'].map(mappings['Education'])\n",
        "\n",
        "   # Combine race columns\n",
        "   race_cols = ['white', 'black', 'native', 'Asian', 'Pacific', 'others']\n",
        "   df['Race'] = df.apply(lambda x: ', '.join(\n",
        "       [race for race, col in zip(race_cols, ['Race_' + r for r in race_cols])\n",
        "        if x[col] == '1'] +\n",
        "       ([x['Race_text']] if pd.notna(x['Race_text']) and x['Race_text'] else [])), axis=1)\n",
        "\n",
        "   # Combine employment columns\n",
        "   emp_mappings = {\n",
        "       'Employment_retired': 'retired',\n",
        "       'Employment_self-employed': 'self-employed',\n",
        "       'Employment_employment full-time': 'full-time',\n",
        "       'Employment_employment part-time': 'part-time',\n",
        "       'Employment_voluntary worker': 'voluntary worker',\n",
        "       'Employment_homemaker': 'homemaker',\n",
        "       'Employment_student': 'student',\n",
        "       'Employment_others': 'others'\n",
        "   }\n",
        "\n",
        "   df['Employment'] = df.apply(lambda x: ', '.join(\n",
        "       [val for col, val in emp_mappings.items() if x[col] == '1'] +\n",
        "       ([x['Employment_text']] if pd.notna(x['Employment_text']) and x['Employment_text'] else [])), axis=1)\n",
        "\n",
        "   # Drop original columns and rename\n",
        "   cols_to_drop = (['Brand_text', 'Gender_text', 'Race_text', 'Employment_text'] +\n",
        "                   ['Race_' + r for r in race_cols] +\n",
        "                   list(emp_mappings.keys()))\n",
        "   df = df.drop(cols_to_drop, axis=1)\n",
        "   df = df.rename(columns={'Employment_hours_text': 'Part_time_hours'})\n",
        "\n",
        "   # Reorder columns\n",
        "   columns_order = [\n",
        "       'Age', 'Gender', 'Education', 'Race', 'Employment', 'Part_time_hours',\n",
        "       'Brand', 'Android_version', 'Screen_time',\n",
        "       'Name', 'Email', 'Source'\n",
        "   ]\n",
        "   return df[columns_order]\n",
        "\n",
        "signup_data3 = process_signup_data(signup_data2)"
      ],
      "metadata": {
        "id": "f8XRv-ELKnfj"
      },
      "id": "f8XRv-ELKnfj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "signup_data3.to_csv(google_drive_data_path + '/signup_survey_data_processed.csv', index=False)"
      ],
      "metadata": {
        "id": "ZSmEOWgfQgAk"
      },
      "id": "ZSmEOWgfQgAk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_MWR5ilwQlu8"
      },
      "id": "_MWR5ilwQlu8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Consent form"
      ],
      "metadata": {
        "id": "vwgkHqxIspAw"
      },
      "id": "vwgkHqxIspAw"
    },
    {
      "cell_type": "code",
      "source": [
        "consert_form_survey = \"SV_1Y79vGshtWh9FPM\"\n",
        "consent_form_data = get_qualtrics_data(api_token, consert_form_survey)\n",
        "\n",
        "with open('consent_form_data.csv', 'w') as f:\n",
        "    f.write(consent_form_data)\n",
        "    f.close()\n",
        "\n",
        "consent_form_data = pd.read_csv('consent_form_data.csv')\n",
        "print(\"The shape of the consent_form_data is \" + str(consent_form_data.shape))\n",
        "consent_form_data.to_csv(google_drive_data_path + '/consent_form_data.csv', index=False)"
      ],
      "metadata": {
        "id": "B2CVTn7C9MNq"
      },
      "id": "B2CVTn7C9MNq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Backup"
      ],
      "metadata": {
        "id": "AU2TzoXMwogx"
      },
      "id": "AU2TzoXMwogx"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unused code. Don't run this cell unless you know what you are doing."
      ],
      "metadata": {
        "id": "bYdA6E1Xwqvq"
      },
      "id": "bYdA6E1Xwqvq"
    },
    {
      "cell_type": "code",
      "source": [
        "# Ringer mode changes, with mixed data structure. Hopefully we don't need this\n",
        "\n",
        "# def firebase_to_csv_log_mode(ref_path, output_file):\n",
        "#     ref = db.reference(ref_path)\n",
        "#     data = ref.get()\n",
        "#     transformed_data = []\n",
        "\n",
        "#     for pid, middle_levels in data.items():\n",
        "#         pid = pid.lower()\n",
        "#         for middle_key, sub_changes in middle_levels.items():\n",
        "#             # process: checking the structure of sub_data (dic or list)\n",
        "#             if isinstance(sub_changes, dict):\n",
        "#                 for sub_key, event in sub_changes.items():\n",
        "#                     if isinstance(event, dict):\n",
        "#                         row = {'pid': pid, 'host':middle_levels, 'label': sub_key,\n",
        "#                             'mode': event.get('mode'),\n",
        "#                             'timestamp': event.get('timestamp')}\n",
        "#                         transformed_data.append(row)\n",
        "#             elif isinstance(sub_changes, list):  # sub_changes -> list\n",
        "#                 for index, event in enumerate(sub_changes):\n",
        "#                     if isinstance(event, dict):\n",
        "#                         row = {'pid': pid,'label': str(index),\n",
        "#                             'mode': event.get('mode'),\n",
        "#                             'timestamp': event.get('timestamp')}\n",
        "#                         transformed_data.append(row)\n",
        "\n",
        "#     df = pd.DataFrame(transformed_data)\n",
        "#     df = df[['pid', 'label', 'mode', 'timestamp']]\n",
        "#     df.to_csv(output_file, index=False)"
      ],
      "metadata": {
        "id": "oe5P18lm2kSi"
      },
      "execution_count": null,
      "outputs": [],
      "id": "oe5P18lm2kSi"
    },
    {
      "cell_type": "code",
      "source": [
        "# def get_city_from_coords(lat, lon):\n",
        "#     result = rg.search((lat, lon))\n",
        "#     if result:\n",
        "#         return f\"{result[0]['name']}, {result[0]['admin1']}\"\n",
        "#     return \"Unknown location\""
      ],
      "metadata": {
        "id": "45EImRPHw3uw"
      },
      "id": "45EImRPHw3uw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from geopy.geocoders import Nominatim\n",
        "# import time\n",
        "\n",
        "# def get_location_info(lat, lon):\n",
        "#     try:\n",
        "#         geolocator = Nominatim(user_agent=\"my_app\")\n",
        "#         location = geolocator.reverse((lat, lon))\n",
        "#         if location:\n",
        "#             address = location.raw['address']\n",
        "#             state = address.get('state', 'Unknown')\n",
        "#             country = address.get('country', 'Unknown')\n",
        "#             return pd.Series([state, country])\n",
        "#         return pd.Series(['Unknown', 'Unknown'])\n",
        "#     except:\n",
        "#         return pd.Series(['Unknown', 'Unknown'])\n",
        "#     finally:\n",
        "#         # Add a small delay to respect rate limits\n",
        "#         time.sleep(1)"
      ],
      "metadata": {
        "id": "b0qBmyhm4XdL"
      },
      "execution_count": null,
      "outputs": [],
      "id": "b0qBmyhm4XdL"
    },
    {
      "cell_type": "code",
      "source": [
        "# #calculate city\n",
        "# signup_data['City'] = signup_data.apply(lambda row: get_city_from_coords(row['LocationLatitude'], row['LocationLongitude']), axis=1)"
      ],
      "metadata": {
        "id": "lHfjfdN50Ox_"
      },
      "execution_count": null,
      "outputs": [],
      "id": "lHfjfdN50Ox_"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "2vwb_EftYtPp",
        "vwgkHqxIspAw",
        "AU2TzoXMwogx"
      ],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}